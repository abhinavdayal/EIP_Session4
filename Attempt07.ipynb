{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavdayal/EIP_Session4/blob/master/Attempt07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3hqoD5ykPTv",
        "colab_type": "code",
        "outputId": "3e256298-e9db-400d-f0e5-761b5d9b1e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwm22Kr6kj7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_CUxY8kpj8",
        "colab_type": "code",
        "outputId": "2d09cf7d-87a7-429c-ece2-396a039e6e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0upPyS9vkrQM",
        "colab_type": "code",
        "outputId": "b9d3ab9d-9094-403e-aba8-338c7fbc1347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ZPtk0CkuIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0KLE46ykv_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True,\n",
        "                 dropout=0):\n",
        "  \n",
        "    #TODO: add dropout\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  use_bias=False,\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if dropout > 0:\n",
        "            x = Dropout(dropout)(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if dropout > 0:\n",
        "            x = Dropout(dropout)(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP3c5jgikx30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v1_block(x, num_filters, downscale=False, first=False):\n",
        "  \n",
        "  # Residual path\n",
        "  y = resnet_layer(inputs=x, num_filters=num_filters, strides = (2 if downscale else 1))\n",
        "  y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n",
        "\n",
        "  # identity. Need to match the output for addition\n",
        "  if first or downscale:\n",
        "    x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides = (2 if downscale else 1))\n",
        "\n",
        "  # add\n",
        "  x = keras.layers.add([x, y])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG1YgWb8kzj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v1(input_shape, res_blocks, num_classes=10):\n",
        "    # Start model definition.\n",
        "    \n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # first convolution before Resnet starts.\n",
        "    # Should this be done, because we dont want to modify the input\n",
        "    # But in actual resnet they are doing 7x7, may be they are not doing BN or activation here!!!\n",
        "    x = resnet_layer(inputs=inputs, num_filters=16)\n",
        "\n",
        "    num_filters = 64\n",
        "    # Instantiate the stack of residual units\n",
        "    for i, blockcount in enumerate(res_blocks):\n",
        "        for layer in range(blockcount):\n",
        "            x = resnet_v1_block(x, num_filters=num_filters, downscale=i>0 and layer==0, first = layer==0 and i==0)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    x = AveragePooling2D(pool_size=4)(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax', use_bias=False,\n",
        "                    kernel_initializer='he_normal')(x)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfA2Jb0dk2Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = round(0.01 * 1/(1 + 0.219 * epoch), 10)\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYRHxbjek_fF",
        "colab_type": "code",
        "outputId": "0053aa4b-21dc-4561-c1de-0687252f7c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = resnet_v1(input_shape=input_shape, res_blocks=(2, 2, 2, 2))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.01\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   432         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1024        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   36864       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 64)   0           activation_2[0][0]               \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   36864       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36864       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73728       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 128)  8192        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 128)  512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147456      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 128)  0           activation_7[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 128)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 128)  147456      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           activation_8[0][0]               \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 256)    294912      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 256)    1024        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 256)    32768       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 256)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 256)    1024        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 256)    589824      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 256)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 256)    1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 256)    0           activation_12[0][0]              \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 256)    0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 256)    589824      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 256)    1024        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 256)    589824      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 256)    0           activation_13[0][0]              \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 256)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 512)    1179648     activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 512)    2048        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 512)    131072      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 512)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 512)    2048        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 512)    2359296     activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 4, 4, 512)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 512)    2048        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 512)    0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 4, 4, 512)    0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 512)    2359296     activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 4, 4, 512)    2048        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 4, 4, 512)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 512)    2359296     activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 4, 4, 512)    2048        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 512)    0           activation_18[0][0]              \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 4, 4, 512)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 512)    0           activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 512)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5120        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 11,155,696\n",
            "Trainable params: 11,146,064\n",
            "Non-trainable params: 9,632\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtchr4qplFHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_Tesnet18.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DTRqW8vlIf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNLF81qOlO54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w # area of cutout\n",
        "            r = np.random.uniform(r_1, r_2) # \n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ogyrWXilQUA",
        "colab_type": "code",
        "outputId": "0cfe1cfd-7b8f-4b6f-cb7b-a483ccd75cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Moa3tBlUIn",
        "colab_type": "code",
        "outputId": "4bcc8d15-0f3f-4be6-d663-646cf99cb710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks, use_multiprocessing=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.01\n",
            "Epoch 1/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 2.5276 - accuracy: 0.3693WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24337, saving model to /content/saved_models/cifar10_Tesnet18.001.h5\n",
            "390/390 [==============================] - 111s 284ms/step - loss: 2.5286 - accuracy: 0.3692 - val_loss: 3.2113 - val_accuracy: 0.2434\n",
            "Learning rate:  0.0082034454\n",
            "Epoch 2/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.9854 - accuracy: 0.4522WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.24337 to 0.29292, saving model to /content/saved_models/cifar10_Tesnet18.002.h5\n",
            "390/390 [==============================] - 112s 288ms/step - loss: 1.9841 - accuracy: 0.4525 - val_loss: 2.6406 - val_accuracy: 0.2929\n",
            "Learning rate:  0.0069541029\n",
            "Epoch 3/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.5189 - accuracy: 0.5302WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.29292 to 0.41683, saving model to /content/saved_models/cifar10_Tesnet18.003.h5\n",
            "390/390 [==============================] - 112s 287ms/step - loss: 1.5189 - accuracy: 0.5301 - val_loss: 1.9165 - val_accuracy: 0.4168\n",
            "Learning rate:  0.006035003\n",
            "Epoch 4/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.3540 - accuracy: 0.5934WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.41683\n",
            "390/390 [==============================] - 111s 283ms/step - loss: 1.3539 - accuracy: 0.5935 - val_loss: 2.4550 - val_accuracy: 0.3433\n",
            "Learning rate:  0.0053304904\n",
            "Epoch 5/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.2574 - accuracy: 0.6283WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.41683 to 0.54935, saving model to /content/saved_models/cifar10_Tesnet18.005.h5\n",
            "390/390 [==============================] - 111s 286ms/step - loss: 1.2573 - accuracy: 0.6284 - val_loss: 1.5423 - val_accuracy: 0.5493\n",
            "Learning rate:  0.0047732697\n",
            "Epoch 6/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.1661 - accuracy: 0.6640WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.54935 to 0.55528, saving model to /content/saved_models/cifar10_Tesnet18.006.h5\n",
            "390/390 [==============================] - 111s 285ms/step - loss: 1.1660 - accuracy: 0.6641 - val_loss: 1.5479 - val_accuracy: 0.5553\n",
            "Learning rate:  0.0043215212\n",
            "Epoch 7/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0919 - accuracy: 0.6891WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.55528\n",
            "390/390 [==============================] - 110s 282ms/step - loss: 1.0918 - accuracy: 0.6892 - val_loss: 1.5994 - val_accuracy: 0.5341\n",
            "Learning rate:  0.0039478879\n",
            "Epoch 8/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 1.0173 - accuracy: 0.7147WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.55528 to 0.65922, saving model to /content/saved_models/cifar10_Tesnet18.008.h5\n",
            "390/390 [==============================] - 111s 284ms/step - loss: 1.0170 - accuracy: 0.7147 - val_loss: 1.1938 - val_accuracy: 0.6592\n",
            "Learning rate:  0.0036337209\n",
            "Epoch 9/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9638 - accuracy: 0.7335WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.65922 to 0.71222, saving model to /content/saved_models/cifar10_Tesnet18.009.h5\n",
            "390/390 [==============================] - 110s 282ms/step - loss: 0.9640 - accuracy: 0.7333 - val_loss: 1.0667 - val_accuracy: 0.7122\n",
            "Learning rate:  0.0033658701\n",
            "Epoch 10/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.7493WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.71222\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.9143 - accuracy: 0.7491 - val_loss: 1.5979 - val_accuracy: 0.5757\n",
            "Learning rate:  0.0031347962\n",
            "Epoch 11/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8785 - accuracy: 0.7591WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.71222\n",
            "390/390 [==============================] - 110s 281ms/step - loss: 0.8785 - accuracy: 0.7591 - val_loss: 1.2370 - val_accuracy: 0.6662\n",
            "Learning rate:  0.0029334116\n",
            "Epoch 12/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8414 - accuracy: 0.7723WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.71222 to 0.76642, saving model to /content/saved_models/cifar10_Tesnet18.012.h5\n",
            "390/390 [==============================] - 109s 281ms/step - loss: 0.8411 - accuracy: 0.7724 - val_loss: 0.8441 - val_accuracy: 0.7664\n",
            "Learning rate:  0.0027563396\n",
            "Epoch 13/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.8135 - accuracy: 0.7801WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.76642\n",
            "390/390 [==============================] - 110s 281ms/step - loss: 0.8135 - accuracy: 0.7802 - val_loss: 0.9705 - val_accuracy: 0.7291\n",
            "Learning rate:  0.0025994281\n",
            "Epoch 14/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7902 - accuracy: 0.7878WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.76642\n",
            "390/390 [==============================] - 109s 281ms/step - loss: 0.7904 - accuracy: 0.7877 - val_loss: 1.1280 - val_accuracy: 0.7141\n",
            "Learning rate:  0.0024594196\n",
            "Epoch 15/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7658 - accuracy: 0.7971WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.76642\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.7657 - accuracy: 0.7972 - val_loss: 0.9658 - val_accuracy: 0.7449\n",
            "Learning rate:  0.0023337223\n",
            "Epoch 16/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7465 - accuracy: 0.8024WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.76642 to 0.76652, saving model to /content/saved_models/cifar10_Tesnet18.016.h5\n",
            "390/390 [==============================] - 110s 281ms/step - loss: 0.7464 - accuracy: 0.8025 - val_loss: 0.8701 - val_accuracy: 0.7665\n",
            "Learning rate:  0.0022202487\n",
            "Epoch 17/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7189 - accuracy: 0.8116WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.76652 to 0.77581, saving model to /content/saved_models/cifar10_Tesnet18.017.h5\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.7189 - accuracy: 0.8115 - val_loss: 0.8830 - val_accuracy: 0.7758\n",
            "Learning rate:  0.0021172983\n",
            "Epoch 18/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.7085 - accuracy: 0.8121WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.77581\n",
            "390/390 [==============================] - 107s 275ms/step - loss: 0.7085 - accuracy: 0.8120 - val_loss: 0.9671 - val_accuracy: 0.7485\n",
            "Learning rate:  0.0020234723\n",
            "Epoch 19/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.8184WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.77581\n",
            "390/390 [==============================] - 107s 275ms/step - loss: 0.6878 - accuracy: 0.8185 - val_loss: 1.1005 - val_accuracy: 0.7068\n",
            "Learning rate:  0.001937609\n",
            "Epoch 20/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.8193WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.77581\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.6812 - accuracy: 0.8193 - val_loss: 1.1243 - val_accuracy: 0.7139\n",
            "Learning rate:  0.0018587361\n",
            "Epoch 21/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6630 - accuracy: 0.8254WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.77581 to 0.79134, saving model to /content/saved_models/cifar10_Tesnet18.021.h5\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.6627 - accuracy: 0.8256 - val_loss: 0.8444 - val_accuracy: 0.7913\n",
            "Learning rate:  0.0017860332\n",
            "Epoch 22/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6464 - accuracy: 0.8329WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.79134 to 0.82437, saving model to /content/saved_models/cifar10_Tesnet18.022.h5\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.6463 - accuracy: 0.8329 - val_loss: 0.6923 - val_accuracy: 0.8244\n",
            "Learning rate:  0.0017188037\n",
            "Epoch 23/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6373 - accuracy: 0.8350WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.82437\n",
            "390/390 [==============================] - 108s 278ms/step - loss: 0.6371 - accuracy: 0.8351 - val_loss: 0.8586 - val_accuracy: 0.8026\n",
            "Learning rate:  0.0016564519\n",
            "Epoch 24/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6306 - accuracy: 0.8365WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.82437\n",
            "390/390 [==============================] - 108s 278ms/step - loss: 0.6306 - accuracy: 0.8364 - val_loss: 1.0729 - val_accuracy: 0.7329\n",
            "Learning rate:  0.0015984655\n",
            "Epoch 25/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6159 - accuracy: 0.8396WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.82437\n",
            "390/390 [==============================] - 108s 276ms/step - loss: 0.6156 - accuracy: 0.8397 - val_loss: 0.7533 - val_accuracy: 0.7963\n",
            "Learning rate:  0.0015444015\n",
            "Epoch 26/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.6046 - accuracy: 0.8435WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.82437\n",
            "390/390 [==============================] - 108s 277ms/step - loss: 0.6047 - accuracy: 0.8434 - val_loss: 0.7077 - val_accuracy: 0.8177\n",
            "Learning rate:  0.0014938751\n",
            "Epoch 27/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.8442WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.82437 to 0.84642, saving model to /content/saved_models/cifar10_Tesnet18.027.h5\n",
            "390/390 [==============================] - 108s 276ms/step - loss: 0.5973 - accuracy: 0.8443 - val_loss: 0.6293 - val_accuracy: 0.8464\n",
            "Learning rate:  0.00144655\n",
            "Epoch 28/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5877 - accuracy: 0.8471WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 107s 275ms/step - loss: 0.5878 - accuracy: 0.8470 - val_loss: 0.7598 - val_accuracy: 0.8027\n",
            "Learning rate:  0.0014021312\n",
            "Epoch 29/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.8486WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.5835 - accuracy: 0.8486 - val_loss: 0.6778 - val_accuracy: 0.8232\n",
            "Learning rate:  0.0013603591\n",
            "Epoch 30/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5649 - accuracy: 0.8558WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.5648 - accuracy: 0.8558 - val_loss: 0.7074 - val_accuracy: 0.8146\n",
            "Learning rate:  0.001321004\n",
            "Epoch 31/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5620 - accuracy: 0.8558WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.5618 - accuracy: 0.8558 - val_loss: 0.6330 - val_accuracy: 0.8318\n",
            "Learning rate:  0.0012838619\n",
            "Epoch 32/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5520 - accuracy: 0.8571WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 108s 278ms/step - loss: 0.5516 - accuracy: 0.8573 - val_loss: 0.6396 - val_accuracy: 0.8383\n",
            "Learning rate:  0.0012487512\n",
            "Epoch 33/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.8614WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.5420 - accuracy: 0.8614 - val_loss: 0.7244 - val_accuracy: 0.8137\n",
            "Learning rate:  0.0012155099\n",
            "Epoch 34/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5371 - accuracy: 0.8608WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 109s 278ms/step - loss: 0.5369 - accuracy: 0.8609 - val_loss: 0.6168 - val_accuracy: 0.8442\n",
            "Learning rate:  0.0011839924\n",
            "Epoch 35/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5307 - accuracy: 0.8637WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 108s 277ms/step - loss: 0.5306 - accuracy: 0.8637 - val_loss: 0.6233 - val_accuracy: 0.8421\n",
            "Learning rate:  0.0011540681\n",
            "Epoch 36/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5296 - accuracy: 0.8631WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 108s 278ms/step - loss: 0.5295 - accuracy: 0.8631 - val_loss: 0.6630 - val_accuracy: 0.8285\n",
            "Learning rate:  0.0011256191\n",
            "Epoch 37/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5197 - accuracy: 0.8669WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 108s 277ms/step - loss: 0.5196 - accuracy: 0.8669 - val_loss: 0.6407 - val_accuracy: 0.8374\n",
            "Learning rate:  0.0010985389\n",
            "Epoch 38/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5139 - accuracy: 0.8677WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.84642\n",
            "390/390 [==============================] - 108s 278ms/step - loss: 0.5141 - accuracy: 0.8676 - val_loss: 0.6085 - val_accuracy: 0.8377\n",
            "Learning rate:  0.0010727312\n",
            "Epoch 39/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.8698WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.84642 to 0.86551, saving model to /content/saved_models/cifar10_Tesnet18.039.h5\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.5117 - accuracy: 0.8698 - val_loss: 0.5549 - val_accuracy: 0.8655\n",
            "Learning rate:  0.0010481082\n",
            "Epoch 40/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.8737WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 108s 277ms/step - loss: 0.4964 - accuracy: 0.8737 - val_loss: 0.6305 - val_accuracy: 0.8421\n",
            "Learning rate:  0.0010245902\n",
            "Epoch 41/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.5001 - accuracy: 0.8719WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 278ms/step - loss: 0.4999 - accuracy: 0.8719 - val_loss: 0.6962 - val_accuracy: 0.8182\n",
            "Learning rate:  0.0010021044\n",
            "Epoch 42/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4848 - accuracy: 0.8748WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.4847 - accuracy: 0.8748 - val_loss: 0.6274 - val_accuracy: 0.8358\n",
            "Learning rate:  0.0009805844\n",
            "Epoch 43/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.8754WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 108s 278ms/step - loss: 0.4851 - accuracy: 0.8753 - val_loss: 0.8029 - val_accuracy: 0.7991\n",
            "Learning rate:  0.0009599693\n",
            "Epoch 44/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4775 - accuracy: 0.8773WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.4774 - accuracy: 0.8773 - val_loss: 0.6614 - val_accuracy: 0.8251\n",
            "Learning rate:  0.0009402031\n",
            "Epoch 45/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4728 - accuracy: 0.8791WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.4727 - accuracy: 0.8791 - val_loss: 0.6320 - val_accuracy: 0.8457\n",
            "Learning rate:  0.0009212345\n",
            "Epoch 46/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.8825WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.4670 - accuracy: 0.8824 - val_loss: 0.5658 - val_accuracy: 0.8582\n",
            "Learning rate:  0.0009030161\n",
            "Epoch 47/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4690 - accuracy: 0.8809WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.4691 - accuracy: 0.8809 - val_loss: 0.5845 - val_accuracy: 0.8457\n",
            "Learning rate:  0.0008855043\n",
            "Epoch 48/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4640 - accuracy: 0.8794WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 280ms/step - loss: 0.4640 - accuracy: 0.8794 - val_loss: 0.6042 - val_accuracy: 0.8532\n",
            "Learning rate:  0.0008686588\n",
            "Epoch 49/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4536 - accuracy: 0.8842WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 279ms/step - loss: 0.4535 - accuracy: 0.8843 - val_loss: 0.5744 - val_accuracy: 0.8603\n",
            "Learning rate:  0.0008524422\n",
            "Epoch 50/50\n",
            "389/390 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.8853WARNING:tensorflow:Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence` class.\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.86551\n",
            "390/390 [==============================] - 109s 278ms/step - loss: 0.4536 - accuracy: 0.8853 - val_loss: 0.5669 - val_accuracy: 0.8616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb0e81e56a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQrZ0ouElaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}